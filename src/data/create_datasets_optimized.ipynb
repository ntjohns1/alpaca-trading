{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ece7118",
   "metadata": {},
   "source": [
    "# Create Datasets (Optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0f29c",
   "metadata": {},
   "source": [
    "Create an HDF5 table from SHARADAR data that mirrors the WIKI_PRICES.csv format.\n",
    "This script combines data from SHARADAR_SEP.csv (price data) and SHARADAR_ACTIONS.csv\n",
    "(dividend and split information) to create a dataset compatible with the format used\n",
    "in the ML4T examples.\n",
    "\n",
    "This version is optimized for memory efficiency to prevent kernel crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import gc  # For garbage collection\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/home/noslen/alpaca-trading/data')\n",
    "SHARADAR_DIR = DATA_DIR / 'SHARADAR'\n",
    "OUTPUT_FILE = DATA_DIR / 'assets.h5'\n",
    "WIKI_PRICES_PATH = DATA_DIR / 'WIKI_PRICES.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3dd04",
   "metadata": {},
   "source": [
    "### Load SHARADAR_SEP.csv price data (with chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b747d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_SEP.csv: 17300827\n",
      "Preview of data:\n",
      "   ticker       date  open  high   low  close   volume  closeadj  closeunadj  \\\n",
      "0   ABILF 2021-11-09  0.30  0.33  0.30   0.33   7500.0      0.33        0.33   \n",
      "1   ABILF 2021-11-08  0.35  0.35  0.35   0.35      0.0      0.35        0.35   \n",
      "2     AAC 2021-09-24  9.74  9.75  9.73   9.75  38502.0      9.75        9.75   \n",
      "3   AAC.U 2021-09-24  9.95  9.95  9.90   9.90   2692.0      9.90        9.90   \n",
      "4  AAC.WS 2021-09-24  0.92  0.92  0.87   0.89  38784.0      0.89        0.89   \n",
      "\n",
      "  lastupdated  \n",
      "0  2021-11-09  \n",
      "1  2021-11-09  \n",
      "2  2021-09-24  \n",
      "3  2021-09-24  \n",
      "4  2021-09-24  \n"
     ]
    }
   ],
   "source": [
    "# Check the total number of rows first\n",
    "sep_path = SHARADAR_DIR / 'SHARADAR_SEP.csv'\n",
    "row_count = sum(1 for _ in open(sep_path)) - 1  # Subtract 1 for header\n",
    "print(f\"Total rows in {sep_path}: {row_count}\")\n",
    "\n",
    "# Define a reasonable chunk size (adjust based on your system's memory)\n",
    "CHUNK_SIZE = 500000  # Process 500,000 rows at a time\n",
    "\n",
    "# Preview the first few rows\n",
    "preview_df = pd.read_csv(sep_path, nrows=5, parse_dates=['date'])\n",
    "print(\"Preview of data:\")\n",
    "print(preview_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a8065",
   "metadata": {},
   "source": [
    "### Load SHARADAR_ACTIONS.csv for dividend and split information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8875346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date         action ticker                               name  \\\n",
      "0 2015-01-02       delisted   XWES         WORLD ENERGY SOLUTIONS INC   \n",
      "1 2015-01-02  acquisitionby   XWES         WORLD ENERGY SOLUTIONS INC   \n",
      "2 2015-01-02       dividend    WSR                    WHITESTONE REIT   \n",
      "3 2015-01-02       dividend   WSCI                 WSI INDUSTRIES INC   \n",
      "4 2015-01-02          split  WMLPQ  WESTMORELAND RESOURCE PARTNERS LP   \n",
      "\n",
      "      value contraticker   contraname  \n",
      "0  69.40000          NaN          NaN  \n",
      "1  69.40000         ENOC  ENERNOC INC  \n",
      "2   0.09500          NaN          NaN  \n",
      "3   0.04000          NaN          NaN  \n",
      "4   0.08333          NaN          NaN  \n",
      "loaded 323840 rows from /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_ACTIONS.csv\n"
     ]
    }
   ],
   "source": [
    "actions_path = SHARADAR_DIR / 'SHARADAR_ACTIONS.csv'\n",
    "    \n",
    "# Actions data should be much smaller, so we can load it all at once\n",
    "actions_df = pd.read_csv(\n",
    "    actions_path,\n",
    "    parse_dates=['date'],\n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "print(actions_df.head())\n",
    "print(f\"loaded {len(actions_df)} rows from {actions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf6a31",
   "metadata": {},
   "source": [
    "### Extract dividend information from actions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f6fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 270003 dividend records\n",
      "        date ticker  ex-dividend\n",
      "0 2015-01-02      A      0.10000\n",
      "1 2015-01-02   ABEV      0.04893\n",
      "2 2015-01-02  ARPJQ      0.19660\n",
      "3 2015-01-02    ATW      0.25000\n",
      "4 2015-01-02    BDN      0.15000\n"
     ]
    }
   ],
   "source": [
    "# Filter for dividend actions\n",
    "dividend_df = actions_df[actions_df['action'] == 'dividend'].copy()\n",
    "\n",
    "# Create a dataframe with ticker, date, and dividend amount\n",
    "dividend_df = dividend_df[['date', 'ticker', 'value']].rename(columns={'value': 'ex-dividend'})\n",
    "# Sum up multiple dividends on the same day for the same ticker\n",
    "dividend_df = dividend_df.groupby(['date', 'ticker']).sum().reset_index()\n",
    "\n",
    "print(f\"Extracted {len(dividend_df)} dividend records\")\n",
    "print(dividend_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dd4f2",
   "metadata": {},
   "source": [
    "### Extract split information from actions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46ddb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4088 split records\n",
      "         date ticker  split_ratio\n",
      "4  2015-01-02  WMLPQ      0.08333\n",
      "12 2015-01-02   RFMD      0.25000\n",
      "22 2015-01-02   GNTX      2.00000\n",
      "59 2015-01-05  PSTRQ      0.10000\n",
      "96 2015-01-07   UBFO      1.01000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for split actions\n",
    "splits = actions_df[actions_df['action'] == 'split'].copy()\n",
    "    \n",
    "# Create a dataframe with ticker, date, and split ratio\n",
    "split_df = splits[['date', 'ticker', 'value']].rename(columns={'value': 'split_ratio'})\n",
    "\n",
    "print(f\"Extracted {len(split_df)} split records\")\n",
    "print(split_df.head())\n",
    "\n",
    "# Free up memory\n",
    "del actions_df, splits\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21253d2",
   "metadata": {},
   "source": [
    "## Transform SHARADAR data into WIKI_PRICES.csv format (processing in chunks)\n",
    "\n",
    "WIKI_PRICES.csv columns:\n",
    "ticker,date,open,high,low,close,volume,ex-dividend,split_ratio,adj_open,adj_high,adj_low,adj_close,adj_volume\n",
    "\n",
    "SHARADAR_SEP.csv columns:\n",
    "ticker,date,open,high,low,close,volume,closeadj,closeunadj,lastupdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2b156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 15:20:23,748 - INFO - Created HDF5 store at /home/noslen/alpaca-trading/data/assets.h5\n",
      "2025-05-31 15:20:24,299 - INFO - Processing chunk 1 with 500000 rows (500000 total rows processed)\n",
      "2025-05-31 15:20:25,856 - INFO - Chunk 1 processed and saved\n",
      "2025-05-31 15:20:26,395 - INFO - Processing chunk 2 with 500000 rows (1000000 total rows processed)\n",
      "2025-05-31 15:20:27,945 - INFO - Chunk 2 processed and saved\n",
      "2025-05-31 15:20:28,483 - INFO - Processing chunk 3 with 500000 rows (1500000 total rows processed)\n",
      "2025-05-31 15:20:30,196 - INFO - Chunk 3 processed and saved\n",
      "2025-05-31 15:20:30,727 - INFO - Processing chunk 4 with 500000 rows (2000000 total rows processed)\n",
      "2025-05-31 15:20:32,212 - INFO - Chunk 4 processed and saved\n",
      "2025-05-31 15:20:32,738 - INFO - Processing chunk 5 with 500000 rows (2500000 total rows processed)\n",
      "2025-05-31 15:20:34,507 - INFO - Chunk 5 processed and saved\n",
      "2025-05-31 15:20:35,027 - INFO - Processing chunk 6 with 500000 rows (3000000 total rows processed)\n",
      "2025-05-31 15:20:36,426 - INFO - Chunk 6 processed and saved\n",
      "2025-05-31 15:20:36,951 - INFO - Processing chunk 7 with 500000 rows (3500000 total rows processed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to store a string with len [9] in [ticker] column but\nthis column has a limit of [8]!\nConsider using min_itemsize to preset the sizes on these columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m         store\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharadar/prices\u001b[39m\u001b[38;5;124m'\u001b[39m, wiki_chunk, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# For subsequent chunks, append to the dataset\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msharadar/prices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwiki_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Free memory\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m wiki_chunk, chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:1317\u001b[0m, in \u001b[0;36mHDFStore.append\u001b[0;34m(self, key, value, format, axes, index, append, complib, complevel, columns, min_itemsize, nan_rep, chunksize, expectedrows, dropna, data_columns, encoding, errors)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.hdf.default_format\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_format(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_to_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpectedrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpectedrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:1858\u001b[0m, in \u001b[0;36mHDFStore._write_to_group\u001b[0;34m(self, key, value, format, axes, index, append, complib, complevel, fletcher32, min_itemsize, chunksize, expectedrows, dropna, nan_rep, data_columns, encoding, errors, track_times)\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression not supported on Fixed format stores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# write the object\u001b[39;00m\n\u001b[0;32m-> 1858\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfletcher32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfletcher32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpectedrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpectedrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, Table) \u001b[38;5;129;01mand\u001b[39;00m index:\n\u001b[1;32m   1875\u001b[0m     s\u001b[38;5;241m.\u001b[39mcreate_index(columns\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:4859\u001b[0m, in \u001b[0;36mAppendableMultiFrameTable.write\u001b[0;34m(self, obj, data_columns, **kwargs)\u001b[0m\n\u001b[1;32m   4857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_columns:\n\u001b[1;32m   4858\u001b[0m         data_columns\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, n)\n\u001b[0;32m-> 4859\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:4397\u001b[0m, in \u001b[0;36mAppendableTable.write\u001b[0;34m(self, obj, axes, append, complib, complevel, fletcher32, min_itemsize, chunksize, expectedrows, dropna, nan_rep, data_columns, track_times)\u001b[0m\n\u001b[1;32m   4394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mremove_node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;66;03m# create the axes\u001b[39;00m\n\u001b[0;32m-> 4397\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4398\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   4407\u001b[0m     a\u001b[38;5;241m.\u001b[39mvalidate_names()\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:4046\u001b[0m, in \u001b[0;36mTable._create_axes\u001b[0;34m(self, axes, obj, validate, nan_rep, data_columns, min_itemsize)\u001b[0m\n\u001b[1;32m   4043\u001b[0m     existing_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4045\u001b[0m new_name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues_block_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4046\u001b[0m data_converted \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_convert_for_string_atom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexisting_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4053\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4056\u001b[0m adj_name \u001b[38;5;241m=\u001b[39m _maybe_adjust_name(new_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion)\n\u001b[1;32m   4058\u001b[0m typ \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m_get_atom(data_converted)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:5120\u001b[0m, in \u001b[0;36m_maybe_convert_for_string_atom\u001b[0;34m(name, bvalues, existing_col, min_itemsize, nan_rep, encoding, errors, columns)\u001b[0m\n\u001b[1;32m   5118\u001b[0m \u001b[38;5;66;03m# check for column in the values conflicts\u001b[39;00m\n\u001b[1;32m   5119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m existing_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5120\u001b[0m     eci \u001b[38;5;241m=\u001b[39m \u001b[43mexisting_col\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitemsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eci \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m eci \u001b[38;5;241m>\u001b[39m itemsize:\n\u001b[1;32m   5122\u001b[0m         itemsize \u001b[38;5;241m=\u001b[39m eci\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:2247\u001b[0m, in \u001b[0;36mIndexCol.validate_col\u001b[0;34m(self, itemsize)\u001b[0m\n\u001b[1;32m   2245\u001b[0m             itemsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m   2246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m<\u001b[39m itemsize:\n\u001b[0;32m-> 2247\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2248\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to store a string with len [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitemsize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2249\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] column but\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mthis column has a limit of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2250\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mitemsize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using min_itemsize to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2251\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreset the sizes on these columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2252\u001b[0m             )\n\u001b[1;32m   2253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to store a string with len [9] in [ticker] column but\nthis column has a limit of [8]!\nConsider using min_itemsize to preset the sizes on these columns"
     ]
    }
   ],
   "source": [
    "# Process the data in chunks to avoid memory issues\n",
    "chunk_reader = pd.read_csv(\n",
    "    sep_path,\n",
    "    parse_dates=['date'],\n",
    "    chunksize=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "# Create an empty HDF5 store\n",
    "with pd.HDFStore(OUTPUT_FILE, mode='w') as store:\n",
    "    logger.info(f\"Created HDF5 store at {OUTPUT_FILE}\")\n",
    "\n",
    "min_itemsize = {'ticker': max_ticker_len}\n",
    "logger.info(f\"Setting min_itemsize to {min_itemsize}\")\n",
    "\n",
    "# Process each chunk\n",
    "chunk_count = 0\n",
    "total_rows_processed = 0\n",
    "\n",
    "for chunk in chunk_reader:\n",
    "    chunk_count += 1\n",
    "    chunk_size = len(chunk)\n",
    "    total_rows_processed += chunk_size\n",
    "    \n",
    "    logger.info(f\"Processing chunk {chunk_count} with {chunk_size} rows ({total_rows_processed} total rows processed)\")\n",
    "    \n",
    "    # Start with the price data from this chunk\n",
    "    wiki_chunk = chunk[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "    \n",
    "    # Add ex-dividend column (default to 0.0)\n",
    "    wiki_chunk['ex-dividend'] = 0.0\n",
    "    \n",
    "    # Add split_ratio column (default to 1.0)\n",
    "    wiki_chunk['split_ratio'] = 1.0\n",
    "\n",
    "    # Update with actual dividend information\n",
    "    if not dividend_df.empty:\n",
    "        # Merge dividend information\n",
    "        wiki_chunk = pd.merge(\n",
    "            wiki_chunk, \n",
    "            dividend_df, \n",
    "            on=['ticker', 'date'], \n",
    "            how='left'\n",
    "        )\n",
    "        # Fill missing values with 0.0 and handle duplicates\n",
    "        wiki_chunk['ex-dividend'] = wiki_chunk['ex-dividend_y'].fillna(wiki_chunk['ex-dividend_x'])\n",
    "        wiki_chunk.drop(['ex-dividend_x', 'ex-dividend_y'], axis=1, inplace=True)\n",
    "\n",
    "    # Update with actual split information\n",
    "    if not split_df.empty:\n",
    "        # Merge split information\n",
    "        wiki_chunk = pd.merge(\n",
    "            wiki_chunk, \n",
    "            split_df, \n",
    "            on=['ticker', 'date'], \n",
    "            how='left'\n",
    "        )\n",
    "        # Fill missing values with 1.0 and handle duplicates\n",
    "        wiki_chunk['split_ratio'] = wiki_chunk['split_ratio_y'].fillna(wiki_chunk['split_ratio_x'])\n",
    "        wiki_chunk.drop(['split_ratio_x', 'split_ratio_y'], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate adjusted values using closeadj/close ratio from SHARADAR\n",
    "    # In SHARADAR, closeadj is already adjusted for both splits and dividends\n",
    "    adj_ratio = chunk['closeadj'] / chunk['close']\n",
    "\n",
    "    wiki_chunk['adj_open'] = chunk['open'] * adj_ratio\n",
    "    wiki_chunk['adj_high'] = chunk['high'] * adj_ratio\n",
    "    wiki_chunk['adj_low'] = chunk['low'] * adj_ratio\n",
    "    wiki_chunk['adj_close'] = chunk['closeadj']\n",
    "    wiki_chunk['adj_volume'] = chunk['volume']  # Volume typically doesn't need adjustment in this context\n",
    "\n",
    "    # Set index to date and ticker for consistency with WIKI_PRICES format\n",
    "    wiki_chunk = wiki_chunk.set_index(['date', 'ticker']).sort_index()\n",
    "    \n",
    "    # Append to HDF5 store\n",
    "    with pd.HDFStore(OUTPUT_FILE, mode='a') as store:\n",
    "        if chunk_count == 1:\n",
    "            # For the first chunk, create the dataset\n",
    "            store.put('sharadar/prices', wiki_chunk, format='table')\n",
    "        else:\n",
    "            # For subsequent chunks, append to the dataset\n",
    "            store.append('sharadar/prices', wiki_chunk, format='table')\n",
    "    \n",
    "    # Free memory\n",
    "    del wiki_chunk, chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    logger.info(f\"Chunk {chunk_count} processed and saved\")\n",
    "\n",
    "logger.info(f\"All {total_rows_processed} rows processed and saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:38:48,615 - INFO - HDF5 store contents: ['/sharadar/prices', '/quandl/wiki/prices']\n",
      "2025-06-01 21:38:48,657 - INFO - Sample of data (first 5 rows):\n",
      "                    open    high    low  close     volume  ex-dividend  \\\n",
      "date       ticker                                                        \n",
      "2015-01-02 A       41.18  41.310  40.37  40.56  1530798.0          0.1   \n",
      "           AAUAF    0.95   0.980   0.94   0.95   182200.0          0.0   \n",
      "           ABCB    25.83  25.855  24.92  25.27    73280.0          0.0   \n",
      "           BMO     70.65  70.870  69.72  70.17   554563.0          0.0   \n",
      "           CA1     30.70  30.890  30.28  30.69  3475466.0          0.0   \n",
      "\n",
      "                   split_ratio   adj_open   adj_high    adj_low  adj_close  \\\n",
      "date       ticker                                                            \n",
      "2015-01-02 A               1.0  37.854938  37.974442  37.110341     37.285   \n",
      "           AAUAF           1.0   0.950000   0.980000   0.940000      0.950   \n",
      "           ABCB            1.0  22.800316  22.822383  21.997053     22.306   \n",
      "           BMO             1.0  43.948589  44.085442  43.370073     43.650   \n",
      "           CA1             1.0  27.251877  27.420537  26.879050     27.243   \n",
      "\n",
      "                   adj_volume  \n",
      "date       ticker              \n",
      "2015-01-02 A        1530798.0  \n",
      "           AAUAF     182200.0  \n",
      "           ABCB       73280.0  \n",
      "           BMO       554563.0  \n",
      "           CA1      3475466.0  \n",
      "2025-06-01 21:38:48,658 - INFO - Total rows in dataset: 17300827\n"
     ]
    }
   ],
   "source": [
    "# Verify the results\n",
    "with pd.HDFStore(OUTPUT_FILE, mode='r') as store:\n",
    "    # Check what's in the store\n",
    "    logger.info(f\"HDF5 store contents: {store.keys()}\")\n",
    "    \n",
    "    # Get info about the dataset\n",
    "    prices = store.select('sharadar/prices', start=0, stop=5)\n",
    "    logger.info(f\"Sample of data (first 5 rows):\\n{prices}\")\n",
    "    \n",
    "    # Get total size\n",
    "    total_size = store.get_storer('sharadar/prices').nrows\n",
    "    logger.info(f\"Total rows in dataset: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:48:00,933 - INFO - Retrieved 2604 rows of AAPL data\n",
      "2025-06-01 21:48:00,935 - INFO - Found 0 AAPL records with adj_close = 0.0\n",
      "2025-06-01 21:48:00,936 - INFO - No AAPL records found with adj_close = 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging if not already configured\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# First, get all AAPL data, then filter for adj_close == 0.0\n",
    "with pd.HDFStore(OUTPUT_FILE, mode='r') as store:\n",
    "    # Step 1: Query just for AAPL data\n",
    "    aapl_data = store.select('sharadar/prices', where='ticker == \"AAPL\"')\n",
    "    logger.info(f\"Retrieved {len(aapl_data)} rows of AAPL data\")\n",
    "    \n",
    "    # Step 2: Filter in memory for adj_close == 0.0\n",
    "    aapl_zero_high = aapl_data[aapl_data['adj_high'] == 0.0]\n",
    "    aapl_zero_low = aapl_data[aapl_data['adj_low'] == 0.0]\n",
    "    aapl_zero_open = aapl_data[aapl_data['adj_open'] == 0.0]\n",
    "    aapl_zero_volume = aapl_data[aapl_data['adj_volume'] == 0.0]\n",
    "\n",
    "    \n",
    "    # Display information about the result\n",
    "    logger.info(f\"Found {len(aapl_zero_high)} AAPL records with adj_high = 0.0\")\n",
    "    logger.info(f\"Found {len(aapl_zero_low)} AAPL records with adj_low = 0.0\")\n",
    "    logger.info(f\"Found {len(aapl_zero_open)} AAPL records with adj_open = 0.0\")\n",
    "    logger.info(f\"Found {len(aapl_zero_volume)} AAPL records with adj_volume = 0.0\")\n",
    "    \n",
    "    # Show the data if any records were found\n",
    "    if len(aapl_zero_high) > 0:\n",
    "        logger.info(f\"Sample of AAPL data with adj_high = 0.0:\\n{aapl_zero_high.head()}\")\n",
    "    else:\n",
    "        logger.info(\"No AAPL records found with adj_close = 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17f04d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb622a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07142d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 15:25:10,385 - INFO - Total rows in /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_SEP.csv: 17300827\n",
      "2025-05-31 15:25:10,389 - INFO - Preview of data:\n",
      "2025-05-31 15:25:10,390 - INFO -    ticker       date  open  high   low  close   volume  closeadj  closeunadj  \\\n",
      "0   ABILF 2021-11-09  0.30  0.33  0.30   0.33   7500.0      0.33        0.33   \n",
      "1   ABILF 2021-11-08  0.35  0.35  0.35   0.35      0.0      0.35        0.35   \n",
      "2     AAC 2021-09-24  9.74  9.75  9.73   9.75  38502.0      9.75        9.75   \n",
      "3   AAC.U 2021-09-24  9.95  9.95  9.90   9.90   2692.0      9.90        9.90   \n",
      "4  AAC.WS 2021-09-24  0.92  0.92  0.87   0.89  38784.0      0.89        0.89   \n",
      "\n",
      "  lastupdated  \n",
      "0  2021-11-09  \n",
      "1  2021-11-09  \n",
      "2  2021-09-24  \n",
      "3  2021-09-24  \n",
      "4  2021-09-24  \n",
      "2025-05-31 15:25:10,737 - INFO - Loaded 323840 rows from /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_ACTIONS.csv\n",
      "2025-05-31 15:25:10,864 - INFO - Extracted 270003 dividend records\n",
      "2025-05-31 15:25:10,886 - INFO - Extracted 4088 split records\n",
      "2025-05-31 15:25:23,373 - INFO - Maximum ticker symbol length: 9.0\n",
      "2025-05-31 15:25:23,373 - INFO - Setting min_itemsize to {'ticker': 11.0}\n",
      "2025-05-31 15:25:23,429 - INFO - Created HDF5 store at /home/noslen/alpaca-trading/data/assets.h5\n",
      "2025-05-31 15:25:23,961 - INFO - Processing chunk 1 with 500000 rows (500000 total rows processed)\n",
      "2025-05-31 15:25:25,508 - INFO - Chunk 1 processed and saved\n",
      "2025-05-31 15:25:26,025 - INFO - Processing chunk 2 with 500000 rows (1000000 total rows processed)\n",
      "2025-05-31 15:25:27,563 - INFO - Chunk 2 processed and saved\n",
      "2025-05-31 15:25:28,075 - INFO - Processing chunk 3 with 500000 rows (1500000 total rows processed)\n",
      "2025-05-31 15:25:29,804 - INFO - Chunk 3 processed and saved\n",
      "2025-05-31 15:25:30,310 - INFO - Processing chunk 4 with 500000 rows (2000000 total rows processed)\n",
      "2025-05-31 15:25:31,793 - INFO - Chunk 4 processed and saved\n",
      "2025-05-31 15:25:32,305 - INFO - Processing chunk 5 with 500000 rows (2500000 total rows processed)\n",
      "2025-05-31 15:25:34,027 - INFO - Chunk 5 processed and saved\n",
      "2025-05-31 15:25:34,534 - INFO - Processing chunk 6 with 500000 rows (3000000 total rows processed)\n",
      "2025-05-31 15:25:35,953 - INFO - Chunk 6 processed and saved\n",
      "2025-05-31 15:25:36,464 - INFO - Processing chunk 7 with 500000 rows (3500000 total rows processed)\n",
      "2025-05-31 15:25:37,942 - INFO - Chunk 7 processed and saved\n",
      "2025-05-31 15:25:38,445 - INFO - Processing chunk 8 with 500000 rows (4000000 total rows processed)\n",
      "2025-05-31 15:25:39,753 - INFO - Chunk 8 processed and saved\n",
      "2025-05-31 15:25:40,273 - INFO - Processing chunk 9 with 500000 rows (4500000 total rows processed)\n",
      "2025-05-31 15:25:41,763 - INFO - Chunk 9 processed and saved\n",
      "2025-05-31 15:25:42,285 - INFO - Processing chunk 10 with 500000 rows (5000000 total rows processed)\n",
      "2025-05-31 15:25:43,963 - INFO - Chunk 10 processed and saved\n",
      "2025-05-31 15:25:44,475 - INFO - Processing chunk 11 with 500000 rows (5500000 total rows processed)\n",
      "2025-05-31 15:25:46,028 - INFO - Chunk 11 processed and saved\n",
      "2025-05-31 15:25:46,543 - INFO - Processing chunk 12 with 500000 rows (6000000 total rows processed)\n",
      "2025-05-31 15:25:48,232 - INFO - Chunk 12 processed and saved\n",
      "2025-05-31 15:25:48,746 - INFO - Processing chunk 13 with 500000 rows (6500000 total rows processed)\n",
      "2025-05-31 15:25:50,219 - INFO - Chunk 13 processed and saved\n",
      "2025-05-31 15:25:50,728 - INFO - Processing chunk 14 with 500000 rows (7000000 total rows processed)\n",
      "2025-05-31 15:25:52,261 - INFO - Chunk 14 processed and saved\n",
      "2025-05-31 15:25:52,775 - INFO - Processing chunk 15 with 500000 rows (7500000 total rows processed)\n",
      "2025-05-31 15:25:54,145 - INFO - Chunk 15 processed and saved\n",
      "2025-05-31 15:25:54,663 - INFO - Processing chunk 16 with 500000 rows (8000000 total rows processed)\n",
      "2025-05-31 15:25:56,109 - INFO - Chunk 16 processed and saved\n",
      "2025-05-31 15:25:56,628 - INFO - Processing chunk 17 with 500000 rows (8500000 total rows processed)\n",
      "2025-05-31 15:25:57,936 - INFO - Chunk 17 processed and saved\n",
      "2025-05-31 15:25:58,454 - INFO - Processing chunk 18 with 500000 rows (9000000 total rows processed)\n",
      "2025-05-31 15:26:00,065 - INFO - Chunk 18 processed and saved\n",
      "2025-05-31 15:26:00,582 - INFO - Processing chunk 19 with 500000 rows (9500000 total rows processed)\n",
      "2025-05-31 15:26:02,270 - INFO - Chunk 19 processed and saved\n",
      "2025-05-31 15:26:02,790 - INFO - Processing chunk 20 with 500000 rows (10000000 total rows processed)\n",
      "2025-05-31 15:26:04,330 - INFO - Chunk 20 processed and saved\n",
      "2025-05-31 15:26:04,841 - INFO - Processing chunk 21 with 500000 rows (10500000 total rows processed)\n",
      "2025-05-31 15:26:06,527 - INFO - Chunk 21 processed and saved\n",
      "2025-05-31 15:26:07,043 - INFO - Processing chunk 22 with 500000 rows (11000000 total rows processed)\n",
      "2025-05-31 15:26:08,513 - INFO - Chunk 22 processed and saved\n",
      "2025-05-31 15:26:09,030 - INFO - Processing chunk 23 with 500000 rows (11500000 total rows processed)\n",
      "2025-05-31 15:26:10,526 - INFO - Chunk 23 processed and saved\n",
      "2025-05-31 15:26:11,049 - INFO - Processing chunk 24 with 500000 rows (12000000 total rows processed)\n",
      "2025-05-31 15:26:12,436 - INFO - Chunk 24 processed and saved\n",
      "2025-05-31 15:26:12,948 - INFO - Processing chunk 25 with 500000 rows (12500000 total rows processed)\n",
      "2025-05-31 15:26:14,380 - INFO - Chunk 25 processed and saved\n",
      "2025-05-31 15:26:14,885 - INFO - Processing chunk 26 with 500000 rows (13000000 total rows processed)\n",
      "2025-05-31 15:26:16,356 - INFO - Chunk 26 processed and saved\n",
      "2025-05-31 15:26:16,849 - INFO - Processing chunk 27 with 500000 rows (13500000 total rows processed)\n",
      "2025-05-31 15:26:18,445 - INFO - Chunk 27 processed and saved\n",
      "2025-05-31 15:26:18,943 - INFO - Processing chunk 28 with 500000 rows (14000000 total rows processed)\n",
      "2025-05-31 15:26:20,673 - INFO - Chunk 28 processed and saved\n",
      "2025-05-31 15:26:21,166 - INFO - Processing chunk 29 with 500000 rows (14500000 total rows processed)\n",
      "2025-05-31 15:26:22,696 - INFO - Chunk 29 processed and saved\n",
      "2025-05-31 15:26:23,183 - INFO - Processing chunk 30 with 500000 rows (15000000 total rows processed)\n",
      "2025-05-31 15:26:24,911 - INFO - Chunk 30 processed and saved\n",
      "2025-05-31 15:26:25,400 - INFO - Processing chunk 31 with 500000 rows (15500000 total rows processed)\n",
      "2025-05-31 15:26:26,863 - INFO - Chunk 31 processed and saved\n",
      "2025-05-31 15:26:27,340 - INFO - Processing chunk 32 with 500000 rows (16000000 total rows processed)\n",
      "2025-05-31 15:26:28,843 - INFO - Chunk 32 processed and saved\n",
      "2025-05-31 15:26:29,306 - INFO - Processing chunk 33 with 500000 rows (16500000 total rows processed)\n",
      "2025-05-31 15:26:30,630 - INFO - Chunk 33 processed and saved\n",
      "2025-05-31 15:26:31,085 - INFO - Processing chunk 34 with 500000 rows (17000000 total rows processed)\n",
      "2025-05-31 15:26:32,402 - INFO - Chunk 34 processed and saved\n",
      "2025-05-31 15:26:32,677 - INFO - Processing chunk 35 with 300827 rows (17300827 total rows processed)\n",
      "2025-05-31 15:26:33,583 - INFO - Chunk 35 processed and saved\n",
      "2025-05-31 15:26:33,584 - INFO - All 17300827 rows processed and saved to /home/noslen/alpaca-trading/data/assets.h5\n",
      "2025-05-31 15:26:33,586 - INFO - HDF5 store contents: ['/sharadar/prices']\n",
      "2025-05-31 15:26:33,604 - INFO - Sample of data (first 5 rows):\n",
      "                    open    high    low  close     volume  ex-dividend  \\\n",
      "date       ticker                                                        \n",
      "2015-01-02 A       41.18  41.310  40.37  40.56  1530798.0          0.1   \n",
      "           AAUAF    0.95   0.980   0.94   0.95   182200.0          0.0   \n",
      "           ABCB    25.83  25.855  24.92  25.27    73280.0          0.0   \n",
      "           BMO     70.65  70.870  69.72  70.17   554563.0          0.0   \n",
      "           CA1     30.70  30.890  30.28  30.69  3475466.0          0.0   \n",
      "\n",
      "                   split_ratio   adj_open   adj_high    adj_low  adj_close  \\\n",
      "date       ticker                                                            \n",
      "2015-01-02 A               1.0  37.854938  37.974442  37.110341     37.285   \n",
      "           AAUAF           1.0   0.950000   0.980000   0.940000      0.950   \n",
      "           ABCB            1.0  22.800316  22.822383  21.997053     22.306   \n",
      "           BMO             1.0  43.948589  44.085442  43.370073     43.650   \n",
      "           CA1             1.0  27.251877  27.420537  26.879050     27.243   \n",
      "\n",
      "                   adj_volume  \n",
      "date       ticker              \n",
      "2015-01-02 A        1530798.0  \n",
      "           AAUAF     182200.0  \n",
      "           ABCB       73280.0  \n",
      "           BMO       554563.0  \n",
      "           CA1      3475466.0  \n",
      "2025-05-31 15:26:33,605 - INFO - Total rows in dataset: 17300827\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create an HDF5 table from SHARADAR data that mirrors the WIKI_PRICES.csv format.\n",
    "This script combines data from SHARADAR_SEP.csv (price data) and SHARADAR_ACTIONS.csv\n",
    "(dividend and split information) to create a dataset compatible with the format used\n",
    "in the ML4T examples.\n",
    "\n",
    "This version is optimized for memory efficiency to prevent kernel crashes and fixes\n",
    "the string length issue with ticker symbols.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import gc  # For garbage collection\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/home/noslen/alpaca-trading/data')\n",
    "SHARADAR_DIR = DATA_DIR / 'SHARADAR'\n",
    "OUTPUT_FILE = DATA_DIR / 'assets.h5'\n",
    "\n",
    "# Define a reasonable chunk size (adjust based on your system's memory)\n",
    "CHUNK_SIZE = 500000  # Process 500,000 rows at a time\n",
    "\n",
    "def main():\n",
    "    # Check the total number of rows first\n",
    "    sep_path = SHARADAR_DIR / 'SHARADAR_SEP.csv'\n",
    "    row_count = sum(1 for _ in open(sep_path)) - 1  # Subtract 1 for header\n",
    "    logger.info(f\"Total rows in {sep_path}: {row_count}\")\n",
    "\n",
    "    # Preview the first few rows\n",
    "    preview_df = pd.read_csv(sep_path, nrows=5, parse_dates=['date'])\n",
    "    logger.info(\"Preview of data:\")\n",
    "    logger.info(preview_df)\n",
    "\n",
    "    # Load actions data (should be much smaller, so we can load it all at once)\n",
    "    actions_path = SHARADAR_DIR / 'SHARADAR_ACTIONS.csv'\n",
    "    actions_df = pd.read_csv(\n",
    "        actions_path,\n",
    "        parse_dates=['date'],\n",
    "        index_col=None\n",
    "    )\n",
    "    logger.info(f\"Loaded {len(actions_df)} rows from {actions_path}\")\n",
    "\n",
    "    # Extract dividend information\n",
    "    dividend_df = actions_df[actions_df['action'] == 'dividend'].copy()\n",
    "    dividend_df = dividend_df[['date', 'ticker', 'value']].rename(columns={'value': 'ex-dividend'})\n",
    "    dividend_df = dividend_df.groupby(['date', 'ticker']).sum().reset_index()\n",
    "    logger.info(f\"Extracted {len(dividend_df)} dividend records\")\n",
    "\n",
    "    # Extract split information\n",
    "    splits = actions_df[actions_df['action'] == 'split'].copy()\n",
    "    split_df = splits[['date', 'ticker', 'value']].rename(columns={'value': 'split_ratio'})\n",
    "    logger.info(f\"Extracted {len(split_df)} split records\")\n",
    "\n",
    "    # Free up memory\n",
    "    del actions_df, splits\n",
    "    gc.collect()\n",
    "\n",
    "    # Process the data in chunks\n",
    "    chunk_reader = pd.read_csv(\n",
    "        sep_path,\n",
    "        parse_dates=['date'],\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "\n",
    "    # Find the maximum ticker length to set min_itemsize properly\n",
    "    max_ticker_len = 0\n",
    "    for chunk in pd.read_csv(sep_path, usecols=['ticker'], chunksize=CHUNK_SIZE):\n",
    "        current_max = chunk['ticker'].str.len().max()\n",
    "        if current_max > max_ticker_len:\n",
    "            max_ticker_len = current_max\n",
    "    \n",
    "    logger.info(f\"Maximum ticker symbol length: {max_ticker_len}\")\n",
    "    \n",
    "    # Add some buffer to be safe\n",
    "    min_itemsize = {'ticker': max_ticker_len + 2}\n",
    "    logger.info(f\"Setting min_itemsize to {min_itemsize}\")\n",
    "\n",
    "    # Create an empty HDF5 store\n",
    "    with pd.HDFStore(OUTPUT_FILE, mode='w') as store:\n",
    "        logger.info(f\"Created HDF5 store at {OUTPUT_FILE}\")\n",
    "\n",
    "    # Process each chunk\n",
    "    chunk_count = 0\n",
    "    total_rows_processed = 0\n",
    "\n",
    "    # Reset the chunk reader\n",
    "    chunk_reader = pd.read_csv(\n",
    "        sep_path,\n",
    "        parse_dates=['date'],\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "\n",
    "    for chunk in chunk_reader:\n",
    "        chunk_count += 1\n",
    "        chunk_size = len(chunk)\n",
    "        total_rows_processed += chunk_size\n",
    "        \n",
    "        logger.info(f\"Processing chunk {chunk_count} with {chunk_size} rows ({total_rows_processed} total rows processed)\")\n",
    "        \n",
    "        # Start with the price data from this chunk\n",
    "        wiki_chunk = chunk[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "        \n",
    "        # Add ex-dividend column (default to 0.0)\n",
    "        wiki_chunk['ex-dividend'] = 0.0\n",
    "        \n",
    "        # Add split_ratio column (default to 1.0)\n",
    "        wiki_chunk['split_ratio'] = 1.0\n",
    "\n",
    "        # Update with actual dividend information\n",
    "        if not dividend_df.empty:\n",
    "            # Merge dividend information\n",
    "            wiki_chunk = pd.merge(\n",
    "                wiki_chunk, \n",
    "                dividend_df, \n",
    "                on=['ticker', 'date'], \n",
    "                how='left'\n",
    "            )\n",
    "            # Fill missing values with 0.0 and handle duplicates\n",
    "            wiki_chunk['ex-dividend'] = wiki_chunk['ex-dividend_y'].fillna(wiki_chunk['ex-dividend_x'])\n",
    "            wiki_chunk.drop(['ex-dividend_x', 'ex-dividend_y'], axis=1, inplace=True)\n",
    "\n",
    "        # Update with actual split information\n",
    "        if not split_df.empty:\n",
    "            # Merge split information\n",
    "            wiki_chunk = pd.merge(\n",
    "                wiki_chunk, \n",
    "                split_df, \n",
    "                on=['ticker', 'date'], \n",
    "                how='left'\n",
    "            )\n",
    "            # Fill missing values with 1.0 and handle duplicates\n",
    "            wiki_chunk['split_ratio'] = wiki_chunk['split_ratio_y'].fillna(wiki_chunk['split_ratio_x'])\n",
    "            wiki_chunk.drop(['split_ratio_x', 'split_ratio_y'], axis=1, inplace=True)\n",
    "\n",
    "        # Calculate adjusted values using closeadj/close ratio from SHARADAR\n",
    "        # In SHARADAR, closeadj is already adjusted for both splits and dividends\n",
    "        adj_ratio = chunk['closeadj'] / chunk['close']\n",
    "\n",
    "        wiki_chunk['adj_open'] = chunk['open'] * adj_ratio\n",
    "        wiki_chunk['adj_high'] = chunk['high'] * adj_ratio\n",
    "        wiki_chunk['adj_low'] = chunk['low'] * adj_ratio\n",
    "        wiki_chunk['adj_close'] = chunk['closeadj']\n",
    "        wiki_chunk['adj_volume'] = chunk['volume']  # Volume typically doesn't need adjustment in this context\n",
    "\n",
    "        # Set index to date and ticker for consistency with WIKI_PRICES format\n",
    "        wiki_chunk = wiki_chunk.set_index(['date', 'ticker']).sort_index()\n",
    "        \n",
    "        # Append to HDF5 store with min_itemsize parameter to handle longer ticker symbols\n",
    "        with pd.HDFStore(OUTPUT_FILE, mode='a') as store:\n",
    "            if chunk_count == 1:\n",
    "                # For the first chunk, create the dataset with min_itemsize\n",
    "                store.put('sharadar/prices', wiki_chunk, format='table', min_itemsize=min_itemsize)\n",
    "            else:\n",
    "                # For subsequent chunks, append to the dataset\n",
    "                store.append('sharadar/prices', wiki_chunk, format='table', min_itemsize=min_itemsize)\n",
    "        \n",
    "        # Free memory\n",
    "        del wiki_chunk, chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        logger.info(f\"Chunk {chunk_count} processed and saved\")\n",
    "\n",
    "    logger.info(f\"All {total_rows_processed} rows processed and saved to {OUTPUT_FILE}\")\n",
    "\n",
    "    # Verify the results\n",
    "    with pd.HDFStore(OUTPUT_FILE, mode='r') as store:\n",
    "        # Check what's in the store\n",
    "        logger.info(f\"HDF5 store contents: {store.keys()}\")\n",
    "        \n",
    "        # Get info about the dataset\n",
    "        prices = store.select('sharadar/prices', start=0, stop=5)\n",
    "        logger.info(f\"Sample of data (first 5 rows):\\n{prices}\")\n",
    "        \n",
    "        # Get total size\n",
    "        total_size = store.get_storer('sharadar/prices').nrows\n",
    "        logger.info(f\"Total rows in dataset: {total_size}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469a3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 15389314 entries, (Timestamp('1962-01-02 00:00:00'), 'ARNC') to (Timestamp('2018-03-27 00:00:00'), 'ZUMZ')\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   open         float64\n",
      " 1   high         float64\n",
      " 2   low          float64\n",
      " 3   close        float64\n",
      " 4   volume       float64\n",
      " 5   ex-dividend  float64\n",
      " 6   split_ratio  float64\n",
      " 7   adj_open     float64\n",
      " 8   adj_high     float64\n",
      " 9   adj_low      float64\n",
      " 10  adj_close    float64\n",
      " 11  adj_volume   float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 1.4+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = (pd.read_csv(WIKI_PRICES_PATH,\n",
    "                 parse_dates=['date'],\n",
    "                 index_col=['date', 'ticker'],\n",
    "                 infer_datetime_format=True)\n",
    "     .sort_index())\n",
    "\n",
    "print(df.info())\n",
    "with pd.HDFStore(OUTPUT_FILE) as store:\n",
    "    store.put('quandl/wiki/prices', df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
