{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ece7118",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0f29c",
   "metadata": {},
   "source": [
    "Create an HDF5 table from SHARADAR data that mirrors the WIKI_PRICES.csv format.\n",
    "This script combines data from SHARADAR_SEP.csv (price data) and SHARADAR_ACTIONS.csv\n",
    "(dividend and split information) to create a dataset compatible with the format used\n",
    "in the ML4T examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b21b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/home/noslen/alpaca-trading/data')\n",
    "SHARADAR_DIR = DATA_DIR / 'SHARADAR'\n",
    "OUTPUT_FILE = DATA_DIR / 'assets.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3dd04",
   "metadata": {},
   "source": [
    "### Load SHARADAR_SEP.csv price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b747d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticker       date  open  high   low  close   volume  closeadj  closeunadj  \\\n",
      "0   ABILF 2021-11-09  0.30  0.33  0.30   0.33   7500.0      0.33        0.33   \n",
      "1   ABILF 2021-11-08  0.35  0.35  0.35   0.35      0.0      0.35        0.35   \n",
      "2     AAC 2021-09-24  9.74  9.75  9.73   9.75  38502.0      9.75        9.75   \n",
      "3   AAC.U 2021-09-24  9.95  9.95  9.90   9.90   2692.0      9.90        9.90   \n",
      "4  AAC.WS 2021-09-24  0.92  0.92  0.87   0.89  38784.0      0.89        0.89   \n",
      "\n",
      "  lastupdated  \n",
      "0  2021-11-09  \n",
      "1  2021-11-09  \n",
      "2  2021-09-24  \n",
      "3  2021-09-24  \n",
      "4  2021-09-24  \n",
      "loaded 17300827 rows from /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_SEP.csv\n"
     ]
    }
   ],
   "source": [
    "sep_path = SHARADAR_DIR / 'SHARADAR_SEP.csv'\n",
    "sep_df = pd.read_csv(\n",
    "    sep_path,\n",
    "    parse_dates=['date'],\n",
    "    index_col=None\n",
    ")\n",
    "print(sep_df.head())\n",
    "print(f\"loaded {len(sep_df)} rows from {sep_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a8065",
   "metadata": {},
   "source": [
    "### Load SHARADAR_ACTIONS.csv for dividend and split information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8875346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date         action ticker                               name  \\\n",
      "0 2015-01-02       delisted   XWES         WORLD ENERGY SOLUTIONS INC   \n",
      "1 2015-01-02  acquisitionby   XWES         WORLD ENERGY SOLUTIONS INC   \n",
      "2 2015-01-02       dividend    WSR                    WHITESTONE REIT   \n",
      "3 2015-01-02       dividend   WSCI                 WSI INDUSTRIES INC   \n",
      "4 2015-01-02          split  WMLPQ  WESTMORELAND RESOURCE PARTNERS LP   \n",
      "\n",
      "      value contraticker   contraname  \n",
      "0  69.40000          NaN          NaN  \n",
      "1  69.40000         ENOC  ENERNOC INC  \n",
      "2   0.09500          NaN          NaN  \n",
      "3   0.04000          NaN          NaN  \n",
      "4   0.08333          NaN          NaN  \n",
      "loaded 323840 rows from /home/noslen/alpaca-trading/data/SHARADAR/SHARADAR_ACTIONS.csv\n"
     ]
    }
   ],
   "source": [
    "actions_path = SHARADAR_DIR / 'SHARADAR_ACTIONS.csv'\n",
    "    \n",
    "actions_df = pd.read_csv(\n",
    "    actions_path,\n",
    "    parse_dates=['date'],\n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "print(actions_df.head())\n",
    "print(f\"loaded {len(actions_df)} rows from {actions_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf6a31",
   "metadata": {},
   "source": [
    "### Extract dividend information from actions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f6fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for dividend actions\n",
    "dividends = actions_df[actions_df['action'] == 'dividend'].copy()\n",
    "\n",
    "# Create a dataframe with ticker, date, and dividend value\n",
    "dividend_df = dividends[['date', 'ticker', 'value']].rename(columns={'value': 'ex-dividend'})\n",
    "\n",
    "# If there are multiple dividends on the same day for the same ticker, sum them\n",
    "dividend_df = dividend_df.groupby(['date', 'ticker']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dd4f2",
   "metadata": {},
   "source": [
    "### Extract split information from actions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46ddb60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Filter for split actions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43mactions_df\u001b[49m[actions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m### Create a dataframe with ticker, date, and split ratio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m split_df \u001b[38;5;241m=\u001b[39m splits[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actions_df' is not defined"
     ]
    }
   ],
   "source": [
    "### Filter for split actions\n",
    "splits = actions_df[actions_df['action'] == 'split'].copy()\n",
    "    \n",
    "### Create a dataframe with ticker, date, and split ratio\n",
    "split_df = splits[['date', 'ticker', 'value']].rename(columns={'value': 'split_ratio'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21253d2",
   "metadata": {},
   "source": [
    "## Transform SHARADAR data into WIKI_PRICES.csv format\n",
    "\n",
    "WIKI_PRICES.csv columns:\n",
    "ticker,date,open,high,low,close,volume,ex-dividend,split_ratio,adj_open,adj_high,adj_low,adj_close,adj_volume\n",
    "\n",
    "SHARADAR_SEP.csv columns:\n",
    "ticker,date,open,high,low,close,volume,closeadj,closeunadj,lastupdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5e2b156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 15:01:07,371 - INFO - Merging dividend information...\n",
      "2025-05-31 15:01:17,331 - INFO - Merging split information...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Start with the price data\n",
    "wiki_df = sep_df[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "    \n",
    "# Add ex-dividend column (default to 0.0)\n",
    "wiki_df['ex-dividend'] = 0.0\n",
    "    \n",
    "# Add split_ratio column (default to 1.0)\n",
    "wiki_df['split_ratio'] = 1.0\n",
    "\n",
    "# Update with actual dividend information\n",
    "if not dividend_df.empty:\n",
    "    logger.info(\"Merging dividend information...\")\n",
    "    # Merge dividend information\n",
    "    wiki_df = pd.merge(\n",
    "        wiki_df, \n",
    "        dividend_df, \n",
    "        on=['ticker', 'date'], \n",
    "        how='left'\n",
    "    )\n",
    "    # Fill missing values with 0.0 and handle duplicates\n",
    "    wiki_df['ex-dividend'] = wiki_df['ex-dividend_y'].fillna(wiki_df['ex-dividend_x'])\n",
    "    wiki_df.drop(['ex-dividend_x', 'ex-dividend_y'], axis=1, inplace=True)\n",
    "\n",
    "# Update with actual split information\n",
    "if not split_df.empty:\n",
    "    logger.info(\"Merging split information...\")\n",
    "    # Merge split information\n",
    "    wiki_df = pd.merge(\n",
    "        wiki_df, \n",
    "        split_df, \n",
    "        on=['ticker', 'date'], \n",
    "        how='left'\n",
    "    )\n",
    "    # Fill missing values with 1.0 and handle duplicates\n",
    "    wiki_df['split_ratio'] = wiki_df['split_ratio_y'].fillna(wiki_df['split_ratio_x'])\n",
    "    wiki_df.drop(['split_ratio_x', 'split_ratio_y'], axis=1, inplace=True)\n",
    "\n",
    "# Calculate adjusted values using closeadj/close ratio from SHARADAR\n",
    "# In SHARADAR, closeadj is already adjusted for both splits and dividends\n",
    "adj_ratio = sep_df['closeadj'] / sep_df['close']\n",
    "\n",
    "wiki_df['adj_open'] = sep_df['open'] * adj_ratio\n",
    "wiki_df['adj_high'] = sep_df['high'] * adj_ratio\n",
    "wiki_df['adj_low'] = sep_df['low'] * adj_ratio\n",
    "wiki_df['adj_close'] = sep_df['closeadj']\n",
    "wiki_df['adj_volume'] = sep_df['volume']  # Volume typically doesn't need adjustment in this context\n",
    "\n",
    "# Set index to date and ticker for consistency with WIKI_PRICES format\n",
    "wiki_df = wiki_df.set_index(['date', 'ticker']).sort_index()\n",
    "\n",
    "print(wiki_df.info())\n",
    "print(wiki_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f89bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quandl_path = DATA_DIR / 'WIKI_PRICES.csv'\n",
    "# quandl_df = (pd.read_csv(quandl_path,\n",
    "#                  parse_dates=['date'],\n",
    "#                  index_col=['date', 'ticker'],\n",
    "#                  infer_datetime_format=True)\n",
    "#      .sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare wiki_df and quandl_df for duplicate indexes\n",
    "# print(\"Comparing wiki_df and quandl_df for duplicate indexes...\")\n",
    "\n",
    "# # Basic info about both dataframes\n",
    "# print(f\"wiki_df shape: {wiki_df.shape}\")\n",
    "# print(f\"quandl_df shape: {quandl_df.shape}\")\n",
    "\n",
    "# # Get unique tickers and dates in both dataframes\n",
    "# wiki_tickers = wiki_df.index.get_level_values('ticker').unique()\n",
    "# wiki_dates = wiki_df.index.get_level_values('date').unique()\n",
    "# quandl_tickers = quandl_df.index.get_level_values('ticker').unique()\n",
    "# quandl_dates = quandl_df.index.get_level_values('date').unique()\n",
    "\n",
    "# print(f\"wiki_df unique tickers: {len(wiki_tickers)}\")\n",
    "# print(f\"wiki_df date range: {wiki_dates.min()} to {wiki_dates.max()}\")\n",
    "# print(f\"quandl_df unique tickers: {len(quandl_tickers)}\")\n",
    "# print(f\"quandl_df date range: {quandl_dates.min()} to {quandl_dates.max()}\")\n",
    "\n",
    "# # Find common tickers and dates\n",
    "# common_tickers = set(wiki_tickers).intersection(set(quandl_tickers))\n",
    "# common_dates = set(wiki_dates).intersection(set(quandl_dates))\n",
    "\n",
    "# print(f\"Number of common tickers: {len(common_tickers)}\")\n",
    "# print(f\"Number of common dates: {len(common_dates)}\")\n",
    "\n",
    "# # Check for duplicate indexes (date, ticker pairs)\n",
    "# wiki_indexes = set(wiki_df.index.to_flat_index())\n",
    "# quandl_indexes = set(quandl_df.index.to_flat_index())\n",
    "# duplicate_indexes = wiki_indexes.intersection(quandl_indexes)\n",
    "\n",
    "# print(f\"Number of duplicate indexes (date, ticker pairs): {len(duplicate_indexes)}\")\n",
    "\n",
    "# # If there are duplicates, show a sample\n",
    "# if len(duplicate_indexes) > 0:\n",
    "#     print(\"\\nSample of duplicate indexes:\")\n",
    "#     sample_size = min(5, len(duplicate_indexes))\n",
    "#     sample_duplicates = list(duplicate_indexes)[:sample_size]\n",
    "    \n",
    "#     # Convert tuple indexes back to MultiIndex for easier comparison\n",
    "#     sample_idx = pd.MultiIndex.from_tuples(sample_duplicates, names=['date', 'ticker'])\n",
    "    \n",
    "#     print(\"\\nQuandl data for duplicates:\")\n",
    "#     print(quandl_df.loc[sample_idx])\n",
    "    \n",
    "#     print(\"\\nWiki data for duplicates:\")\n",
    "#     print(wiki_df.loc[sample_idx])\n",
    "    \n",
    "#     # Compare values for duplicate indexes\n",
    "#     print(\"\\nComparing values for duplicate indexes...\")\n",
    "#     for idx in sample_duplicates:\n",
    "#         quandl_row = quandl_df.loc[idx]\n",
    "#         wiki_row = wiki_df.loc[idx]\n",
    "        \n",
    "#         # Compare common columns\n",
    "#         common_cols = set(quandl_df.columns).intersection(set(wiki_df.columns))\n",
    "#         for col in common_cols:\n",
    "#             quandl_val = quandl_row[col]\n",
    "#             wiki_val = wiki_row[col]\n",
    "            \n",
    "#             if isinstance(quandl_val, (float, int)) and isinstance(wiki_val, (float, int)):\n",
    "#                 if not np.isclose(quandl_val, wiki_val, rtol=1e-5):\n",
    "#                     print(f\"Different values for {idx}, column {col}: Quandl={quandl_val}, Wiki={wiki_val}\")\n",
    "#             elif quandl_val != wiki_val:\n",
    "#                 print(f\"Different values for {idx}, column {col}: Quandl={quandl_val}, Wiki={wiki_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.HDFStore(OUTPUT_FILE) as store:\n",
    "#     store.put('quandl/wiki/prices', quandl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604aa4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pytables'.  Use pip or conda to install pytables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/tables/__init__.py:44\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Necessary imports to get versions stored on the cython extension\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilsextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_hdf5_version \u001b[38;5;28;01mas\u001b[39;00m _get_hdf5_version\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "\u001b[0;31mImportError\u001b[0m: libblosc2.so.2: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Use the same path as in the examples: 'sharadar/prices'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     store\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharadar/prices\u001b[39m\u001b[38;5;124m'\u001b[39m, wiki_df)\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharadar/prices\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/io/pytables.py:566\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat is not a defined argument for HDFStore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 566\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m complib \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplib only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m compression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.9/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pytables'.  Use pip or conda to install pytables."
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(OUTPUT_FILE) as store:\n",
    "    # Use the same path as in the examples: 'sharadar/prices'\n",
    "    store.put('sharadar/prices', wiki_df)\n",
    "    logger.info(f\"Data saved to {OUTPUT_FILE} at path 'sharadar/prices'\")\n",
    "    \n",
    "    # Print information about the stored data\n",
    "    logger.info(\"HDF5 store contents:\")\n",
    "    for item in store.keys():\n",
    "        logger.info(f\"  {item}: {store.get_storer(item)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
